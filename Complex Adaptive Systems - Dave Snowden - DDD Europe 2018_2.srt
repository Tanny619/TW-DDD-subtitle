1
00:00:00,000 --> 00:00:02,816
That's kind like the core concept that you gonna move on to

2
00:00:04,352 --> 00:00:05,888
If you're in a complex world

3
00:00:06,144 --> 00:00:09,216
You have to manage by creating boundary conditions

4
00:00:09,984 --> 00:00:13,056
Running experiments and responding to what happens

5
00:00:14,336 --> 00:00:17,152
If you're in an ordered system you can plan and design

6
00:00:17,408 --> 00:00:19,712
And Chaos is something you can use

7
00:00:19,968 --> 00:00:22,016
To get new insight but you want to avoid

8
00:00:22,528 --> 00:00:25,600
and say engineering diagrams always assume order.

9
00:00:27,136 --> 00:00:28,928
The ecosystems don't.

10
00:00:29,440 --> 00:00:34,048
And this is from Brian Arthur, the guy's first brought complexity into economics

11
00:00:34,560 --> 00:00:36,608
?Rodger? defining book called

12
00:00:36,864 --> 00:00:38,144
The evolution of technology

13
00:00:38,656 --> 00:00:41,984
The trouble is even scientists don't like complexity very much

14
00:00:42,496 --> 00:00:45,568
which means you're dealing with systems which have perpetual novelty.

15
00:00:47,104 --> 00:00:48,384
constant change.

16
00:00:49,408 --> 00:00:52,736
I sometimes use the metaphor as a children's party to illustrate this:

17
00:00:53,504 --> 00:00:59,648
Yeah you wouldn't manager children's party with key objectives, targets, mission goals, project plans and milestones

18
00:01:00,160 --> 00:01:01,696
because everything would go wrong.

19
00:01:02,464 --> 00:01:04,768
You can see a more elaborate version of that on the web

20
00:01:06,048 --> 00:01:08,352
That from now we get another couple of Simple Rules

21
00:01:09,120 --> 00:01:11,680
If you want a very simple rule about when something is complex

22
00:01:12,192 --> 00:01:15,264
If the evidence supports contradictory hypotheses

23
00:01:16,032 --> 00:01:19,616
and you can't resolve the contradiction within the available time

24
00:01:19,872 --> 00:01:20,896
then it's complex.

25
00:01:22,432 --> 00:01:26,272
This is actually where we move into a massive new approach to conflict resolution

26
00:01:27,296 --> 00:01:30,880
Because the whole point here is nobody is right or wrong.

27
00:01:31,392 --> 00:01:34,720
Because everything we do will change the system anyway.

28
00:01:36,000 --> 00:01:39,840
So something which might be right last time could be wrong this time

29
00:01:40,864 --> 00:01:42,912
Anybody's managed the children's party?

30
00:01:43,680 --> 00:01:46,752
Yeah a football can catalyze a good pattern of play

31
00:01:47,008 --> 00:01:48,800
In the outdoors in summer

32
00:01:49,056 --> 00:01:52,896
But you really don't want to bounce football in your living room in winter

33
00:01:53,408 --> 00:01:55,712
Because the pattern the play will be radically different.

34
00:01:55,968 --> 00:01:56,992
The context is shifted.

35
00:01:57,504 --> 00:01:59,040
Yeah, and damage will be massive.

36
00:01:59,552 --> 00:02:01,600
Yeah, different things will vary in different contexts.

37
00:02:02,880 --> 00:02:03,904
So basically

38
00:02:04,672 --> 00:02:05,952
What we now start to do

39
00:02:06,464 --> 00:02:11,328
is we look at every coherent and coherence is a really important word in complexity

40
00:02:11,840 --> 00:02:13,120
comes from Philosophy of science.

41
00:02:14,656 --> 00:02:19,264
I can say that evolution is coherent to the facts even though we know it's wrong

42
00:02:19,776 --> 00:02:22,336
whereas creationism is incoherent to the fact.

43
00:02:23,616 --> 00:02:27,968
It avoids that you were wrong therefore your hypothesis everything is a hypothesis.

44
00:02:28,480 --> 00:02:31,552
The issue is what's coherent the fact what's incoherent to it

45
00:02:32,832 --> 00:02:35,904
So all I have to do is to agree your idea is coherent.

46
00:02:37,184 --> 00:02:41,280
I don't have to agree you're right in fact that I can be pretty sure you're wrong but I agree

47
00:02:41,536 --> 00:02:43,072
your argument is coherent.

48
00:02:43,840 --> 00:02:46,144
At which point you get a small amount of resource

49
00:02:46,400 --> 00:02:48,960
to run what's called the safe-to-fail experiment.

50
00:02:49,984 --> 00:02:54,080
But in the complex world you run small safe-to-fail experiments

51
00:02:54,592 --> 00:02:57,920
around every single coherent approach.

52
00:02:58,432 --> 00:03:00,224
And then you see what happens

53
00:03:00,736 --> 00:03:02,016
and you stabilize the systems.

54
00:03:03,296 --> 00:03:10,720
You understand the complex system
by interacting with it not by analyzing it or modeling it.

55
00:03:11,744 --> 00:03:13,280
I repeat that because it's key.

56
00:03:13,792 --> 00:03:16,864
You can't understand the complex adaptive system

57
00:03:17,120 --> 00:03:18,912
through modeling or analysis.

58
00:03:19,424 --> 00:03:22,240
You can only understand it by interaction

59
00:03:22,752 --> 00:03:24,800
with real-time feedback loops

60
00:03:25,056 --> 00:03:27,104
over multiple agents.

61
00:03:27,360 --> 00:03:28,640
So you don't get cognitive bias.

62
00:03:29,408 --> 00:03:30,944
This is kind of like 101 science.

63
00:03:31,456 --> 00:03:34,016
And again that's gonna change the way we design and work.

64
00:03:35,040 --> 00:03:38,624
The only thing I can say with absolute certain about a complex adaptive system

65
00:03:39,136 --> 00:03:42,464
is that whatever I do will produce unintended consequences.

66
00:03:43,232 --> 00:03:46,304
If I show this in Australia everybody gets it straight away.

67
00:03:46,816 --> 00:03:48,352
And that's a cane toad

68
00:03:48,864 --> 00:03:51,680
introduced to reduce pest is now the dominant pest.

69
00:03:54,240 --> 00:03:57,312
Now actually that's important to realize, alright?

70
00:03:57,568 --> 00:04:00,384
If you have a complex system and you intervene,

71
00:04:01,152 --> 00:04:03,200
the only thing I can guarantee you

72
00:04:03,456 --> 00:04:05,760
is that you'll have unintended consequences.

73
00:04:07,040 --> 00:04:10,112
That means you are ethically responsible for them

74
00:04:11,136 --> 00:04:12,672
This is a big new thing on government.

75
00:04:13,184 --> 00:04:16,512
If you do a massive intervention and something happens you didn't expect

76
00:04:17,024 --> 00:04:20,608
Well, sorry it's a complex system unexpected things were gonna happen.

77
00:04:21,120 --> 00:04:23,168
It's your responsibility when they do

78
00:04:24,704 --> 00:04:29,312
which means you want to do smaller experiments in parallel much faster.

79
00:04:30,080 --> 00:04:33,664
so the bad unintended consequences you can destroy

80
00:04:33,920 --> 00:04:35,712
and the good ones you can amplify.

81
00:04:36,736 --> 00:04:40,576
Because actually the unintended consequences will be the accepted moments

82
00:04:40,832 --> 00:04:42,880
The discovery of novel ways of working

83
00:04:43,648 --> 00:04:46,208
and that's actually quite important in terms of what we all do.

84
00:04:47,232 --> 00:04:48,000
So from that

85
00:04:48,768 --> 00:04:50,304
we move on to the Cynefin framework

86
00:04:51,584 --> 00:04:52,608
and basically

87
00:04:53,376 --> 00:04:56,448
the Cynefin framework, by the way, is not drawn like this

88
00:04:57,984 --> 00:05:01,568
It's not drawn like this adding that later just to shut me up, alright?

89
00:05:02,336 --> 00:05:03,616
It's actually drawn like this.

90
00:05:07,456 --> 00:05:12,320
Now it's the whole body of theory behind this but basically you don't want a structured model.

91
00:05:12,832 --> 00:05:14,624
Cynefin is a typology

92
00:05:15,136 --> 00:05:18,464
and in full used the framework emerges from the data

93
00:05:18,720 --> 00:05:20,768
The framework doesn't proceed the data

94
00:05:21,280 --> 00:05:22,304
That's really important

95
00:05:23,072 --> 00:05:25,632
If you give people a categorization framework

96
00:05:25,888 --> 00:05:27,680
They'll make things fit the framework

97
00:05:27,936 --> 00:05:28,704
even if they don't.

98
00:05:29,728 --> 00:05:31,776
So when you're working with sense-making

99
00:05:32,032 --> 00:05:36,640
You allow the model boundaries to emerge from the data itself through discovery process

100
00:05:37,152 --> 00:05:39,712
And then you can use it and that's a really important distinction.

101
00:05:40,480 --> 00:05:43,296
So basically Cynefin divided all (?) into two

102
00:05:44,064 --> 00:05:44,832
spaces where

103
00:05:45,344 --> 00:05:46,112
the relationship

104
00:05:46,368 --> 00:05:47,648
between causes and effects is obvious

105
00:05:49,184 --> 00:05:52,768
So for example in civilized countries we drive on the left-hand side of the road.

106
00:05:57,120 --> 00:06:03,008
There's no particular reason for it, I remember when Sweden was civilized until the 1960s.

107
00:06:03,776 --> 00:06:09,920
They had a national referendum where the people voted to stay civilized but the government have the sense to ignore the referendum

108
00:06:10,176 --> 00:06:12,224
that I'm talking about that example lot at the moment.

109
00:06:12,992 --> 00:06:15,296
Yeah and they moved over to driving on the right

110
00:06:16,320 --> 00:06:18,880
It's not a universal, I mean

111
00:06:19,136 --> 00:06:20,672
Yeah you don't drive on the left

112
00:06:20,928 --> 00:06:24,256
In Sweden if a child runs on the road in the UK

113
00:06:24,512 --> 00:06:27,840
If a child runs on the road in front of you. You drive anywhere to avoid them.

114
00:06:29,120 --> 00:06:32,704
So there is still exceptions even in rigidly constrained systems

115
00:06:32,960 --> 00:06:36,032
Are there if you have worked for IBM, it was a bit different. I remember

116
00:06:36,544 --> 00:06:41,664
Something moved on the road in front of me in the middle of a Welsh Mountain Road and I slammed the brakes on if you do

117
00:06:42,176 --> 00:06:48,320
Discovered it was a sheep but the car was a write-off and I phoned up car admin and they said next time kill the sheep

118
00:06:48,576 --> 00:06:50,112
I still slightly worried about that one

119
00:06:50,880 --> 00:06:55,232
Yes, it was kind of hard I know where there was a sheep or kid until much later, but never mind

120
00:06:55,744 --> 00:07:00,864
So basically an obvious domain we got it we can basically Sense-Categorize-Respond

121
00:07:02,144 --> 00:07:03,936
And we can apply best practice

122
00:07:04,448 --> 00:07:05,984
Because there is one right way of doing things

123
00:07:07,776 --> 00:07:12,128
We then move into the complicated domain where most engineering approaches set.

124
00:07:13,664 --> 00:07:15,712
That's actually where there is a relationship

125
00:07:15,968 --> 00:07:18,016
Between cause and effect there is a right answer

126
00:07:18,272 --> 00:07:22,880
But it can only be discovered through analysis or by the deployment of the right type of expert

127
00:07:23,392 --> 00:07:26,208
Expertise as universal by the way

128
00:07:26,464 --> 00:07:29,536
But at this point we can absolutely trust the expert decision

129
00:07:30,816 --> 00:07:34,144
So basically at this point I Sense-Analyze-Respond

130
00:07:35,424 --> 00:07:37,728
And I apply good practice not best practice.

131
00:07:39,520 --> 00:07:41,056
Now I'd gonna argue at the moment

132
00:07:41,824 --> 00:07:44,384
90% of systems architecture

133
00:07:44,640 --> 00:07:47,200
90% of design sits in that domain.

134
00:07:48,480 --> 00:07:52,832
The assumption is there a right hand so we can discover it we can design for it, that's it.

135
00:07:53,856 --> 00:07:55,392
I'll qualify that in a minute.

136
00:07:56,416 --> 00:07:59,488
We then get if we overconstrained a system here

137
00:07:59,744 --> 00:08:01,792
You get this catastrophic failure

138
00:08:02,048 --> 00:08:03,328
Which is why chaos

139
00:08:04,608 --> 00:08:05,888
Is a temporary space

140
00:08:06,400 --> 00:08:08,960
And I get out of it by Act-Sense-Respond

141
00:08:09,472 --> 00:08:10,240
The good news is

142
00:08:10,496 --> 00:08:11,264
I get novelty

143
00:08:12,800 --> 00:08:14,592
There is nothing like a crisis

144
00:08:15,104 --> 00:08:16,384
to produce innovation

145
00:08:17,664 --> 00:08:21,504
If I did an innovation that is all about starvation pressure and perspective shift

146
00:08:21,760 --> 00:08:22,272
If you look at it

147
00:08:22,784 --> 00:08:26,880
when we can't use existing frameworks or existing ways of doing things

148
00:08:27,392 --> 00:08:28,416
Then we get invented

149
00:08:29,696 --> 00:08:32,256
Yeah that goes back to the cognitive entrainment patterns earlier

150
00:08:33,024 --> 00:08:34,816
So basically I‚Äôll say that we get the complex to me

151
00:08:35,328 --> 00:08:36,608
In the complex domain

152
00:08:39,680 --> 00:08:40,960
What I actually do is

153
00:08:41,216 --> 00:08:41,728
Probe

154
00:08:41,984 --> 00:08:42,752
Sense respond

155
00:08:43,520 --> 00:08:45,056
But I probe in parallel

156
00:08:45,312 --> 00:08:46,080
Not in sequence

157
00:08:46,592 --> 00:08:47,872
That's really important

158
00:08:48,896 --> 00:08:51,712
Because if I do one thing it will nearly always work

159
00:08:52,480 --> 00:08:54,016
You know about the Hawthorne effect?

160
00:08:55,296 --> 00:08:57,856
Ok that that's

161
00:08:58,112 --> 00:09:02,464
Cable manufacturers in New York State 1920 artists scientific management

162
00:09:02,976 --> 00:09:06,560
They increase lighting levels and people became more productive that's good news

163
00:09:07,072 --> 00:09:09,120
These days they would now publish book

164
00:09:09,376 --> 00:09:10,144
called Light

165
00:09:10,400 --> 00:09:11,936
The secret to productivity

166
00:09:12,448 --> 00:09:18,080
And have a certificate and training course based on how to switch on and off lights and change light bulbs

167
00:09:18,848 --> 00:09:23,200
Then they had some pretension to be real scientist so they lowered the lighting level

168
00:09:24,480 --> 00:09:25,760
People became more productive

169
00:09:27,040 --> 00:09:30,112
What they actually found is people respond to novelty

170
00:09:30,880 --> 00:09:35,744
Anything done which is new with attention will work the first time

171
00:09:36,256 --> 00:09:38,304
It doesn't mean it will scale or repeat

172
00:09:39,584 --> 00:09:45,216
and you get somebody terrible management books that one of my least favorite at the moment is lean startup if anybody's read that?

173
00:09:45,984 --> 00:09:52,128
Guy goes interviews people he know who's been successful as a whole body problems there because the way you remember things if you‚Äôre

174
00:09:52,384 --> 00:09:52,896
Successful

175
00:09:53,152 --> 00:09:55,712
Is different from the way you remember them if you fail

176
00:09:56,224 --> 00:10:01,088
And both of them are different from the way they actually happened, so you can't just what people say

177
00:10:01,600 --> 00:10:06,720
I remember saying to him at the conference and did you study the companies who failed and he said no why would we do that

178
00:10:07,744 --> 00:10:12,096
I said we did it when I was in IBM with Dorothy Leonard at Harvard Business School

179
00:10:12,352 --> 00:10:14,656
We discovered all the companies have failed

180
00:10:14,912 --> 00:10:18,496
Did exactly the same things as the companies who succeeded

181
00:10:19,776 --> 00:10:21,312
If yes you had a market

182
00:10:22,080 --> 00:10:25,152
So a certain percentage would bound to succeed

183
00:10:26,688 --> 00:10:32,320
And that's the trouble we got this very lightweight research throwing at simplistic recipes it's really dangerous

184
00:10:33,088 --> 00:10:36,416
Here we come back to complexity I probe since respond

185
00:10:36,928 --> 00:10:39,744
And practice here will effectively be emergent

186
00:10:40,512 --> 00:10:41,792
I will always see a pattern

187
00:10:43,584 --> 00:10:47,168
Now important thing in the latest version I can even call the liminal version

188
00:10:47,680 --> 00:10:48,192
Looks like this

189
00:10:49,472 --> 00:10:51,264
Secrets to liminal spaces

190
00:10:52,544 --> 00:10:56,384
Liminality is a really important concept in anthropology

191
00:10:56,896 --> 00:10:59,968
And if you really want to understand humans go and do an anthropology

192
00:11:00,224 --> 00:11:03,296
course, alright? don't go on some cultural conversion course

193
00:11:04,064 --> 00:11:05,600
Most anthropologists

194
00:11:05,856 --> 00:11:08,416
know more about culture in the first year of studies

195
00:11:08,928 --> 00:11:12,000
than cultural change consultants will now in their entire life Style

196
00:11:13,280 --> 00:11:17,375
Basically liminality is a sense of transition if you put on a mask

197
00:11:18,143 --> 00:11:24,287
there is a point where you start to become what the mask represents but there's a state of transition before you do it

198
00:11:25,055 --> 00:11:27,359
and the liminal concept in software design

199
00:11:27,871 --> 00:11:31,199
is about actually not committing until you really have to

200
00:11:31,967 --> 00:11:35,807
The longer you hold things in the state of transition the more options you've got

201
00:11:37,599 --> 00:11:41,439
And actually in software development which is I know far better than architecture

202
00:11:41,695 --> 00:11:43,743
This is actually where things like scrum sit

203
00:11:45,535 --> 00:11:50,655
Because they are linear iterations based on a defined requirement with a degree of ambiguity

204
00:11:51,167 --> 00:11:52,447
Therefore we're going to explore it

205
00:11:53,983 --> 00:11:57,055
That's the trouble we eliminate ambiguity too early on

206
00:11:57,311 --> 00:12:00,127
Because we want to restricted set to do linear experiments with it

207
00:12:01,407 --> 00:12:02,943
In a mixed methods approach

208
00:12:03,455 --> 00:12:08,063
What we start to do is to say and I‚Äôll use development that same as I know that better

209
00:12:08,575 --> 00:12:09,855
here there's nothing wrong with waterfall

210
00:12:13,695 --> 00:12:19,839
Absolutely nothing wrong with Hardy structure processes you can use a TOGAF framework you can use rigid frameworks

211
00:12:20,095 --> 00:12:26,239
You know exactly what you‚Äôve got to deliver you know the resource with an acceptable time frames why would you not take a waterfall approach

212
00:12:28,031 --> 00:12:31,103
Remember working with Telstra(?) on this and Telstra Jesco and agile

213
00:12:31,615 --> 00:12:34,175
Therefore nobody got promoted unless they were agile

214
00:12:34,943 --> 00:12:37,503
So the waterfall guys rang one year sprints

215
00:12:40,575 --> 00:12:43,903
So they could say they were really agile

216
00:12:44,415 --> 00:12:46,975
That's sort of things is very very Common

217
00:12:47,231 --> 00:12:49,023
Human beings are canny

218
00:12:49,279 --> 00:12:50,815
You can't trust the way they use language

219
00:12:51,839 --> 00:12:53,375
um, over here

220
00:12:53,887 --> 00:12:56,959
One of the Big Things We Lost in architecture and elsewhere is the concept

221
00:12:57,215 --> 00:12:57,727
of time boxes

222
00:12:58,239 --> 00:13:00,543
or we got obsessed with two week’s time boxes

223
00:13:01,567 --> 00:13:05,151
Certainly, when I was involved in architect in large decision support systems

224
00:13:05,663 --> 00:13:07,455
we would play around with three months

225
00:13:07,711 --> 00:13:10,015
With a guaranteed delivery at the end of three months

226
00:13:10,527 --> 00:13:13,855
But we very resource or we’d very delivery

227
00:13:14,111 --> 00:13:14,879
To meet the date

228
00:13:15,903 --> 00:13:20,767
I actually gave us a lot of flexibility in definition phase and people looking like forgotten that stuff

229
00:13:21,535 --> 00:13:23,071
but it created the right sort of focus

230
00:13:24,095 --> 00:13:29,215
Over here we're experimenting with radically different approaches and I'll just mention three of them

231
00:13:29,471 --> 00:13:29,983
At this point

232
00:13:30,751 --> 00:13:34,079
one is we’re now starting to look at an articulated needs

233
00:13:35,615 --> 00:13:37,151
Before we even get

234
00:13:37,407 --> 00:13:39,199
to design of any type

235
00:13:40,735 --> 00:13:46,879
What we need to do is to surface the unarticulated needs the things that people don't yet know they were what they want

236
00:13:48,159 --> 00:13:49,439
So one of the ways we do that

237
00:13:49,951 --> 00:13:53,535
Is we get people to interpret all of their technologies into what’s
244
00:13:53,791 --> 00:13:55,327
Call high obstruction metadata

238
00:13:56,095 --> 00:14:01,471
We capture continuous narratives from users about day to day frustrations about their work

239
00:14:02,239 --> 00:14:04,799
And then we merge the two databases together

240
00:14:05,055 --> 00:14:06,335
and we look at clusters

241
00:14:06,591 --> 00:14:08,639
of user anecdotes with technology

242
00:14:09,151 --> 00:14:11,199
and say why are those come together

243
00:14:11,711 --> 00:14:13,247
And that gives us novel ideas

244
00:14:15,039 --> 00:14:17,343
We actually did this with Philips lighting

245
00:14:17,599 --> 00:14:21,439
taking all of the technology capability customer stories

246
00:14:21,951 --> 00:14:23,231
Putting the two together

247
00:14:23,487 --> 00:14:26,559
Five clusters three of which became multi-million dollar businesses

248
00:14:27,839 --> 00:14:31,935
what you're doing is you're forcing accepted moments or insights

249
00:14:32,703 --> 00:14:36,543
By moving up to a level of abstraction, remember I started this on language?

250
00:14:37,567 --> 00:14:40,127
Ok if you don't know it we learnt to paint

251
00:14:40,383 --> 00:14:42,175
Before we learn to speak

252
00:14:42,687 --> 00:14:43,199
as a species

253
00:14:44,735 --> 00:14:47,551
Art comes before language in human evolution

254
00:14:48,831 --> 00:14:53,951
And it turns out art has major evolution advantage it was an accident when it happened

255
00:14:54,207 --> 00:14:57,279
Somebody sketched some pictures on a wall it kind like work for the hunt

256
00:14:57,791 --> 00:14:59,583
But then it started to really work

257
00:14:59,839 --> 00:15:02,143
But if you move up a level of abstraction

258
00:15:02,655 --> 00:15:04,959
You make connections in completely novel ways

259
00:15:06,239 --> 00:15:07,263
Engineers

260
00:15:07,775 --> 00:15:09,311
Who actually are musicians

261
00:15:09,823 --> 00:15:12,895
Will tell you they have their best problem-solving capability

262
00:15:13,151 --> 00:15:14,175
When they play music

263
00:15:15,711 --> 00:15:18,271
Art is critical to human invention

264
00:15:18,783 --> 00:15:21,343
It's why the problem with STEM education is really scary

265
00:15:21,855 --> 00:15:24,671
Because we are reducing human beings down to the level of the art

266
00:15:26,207 --> 00:15:30,303
Other stuff we doing this is about to start with the University of California San Diego

267
00:15:30,559 --> 00:15:33,375
And this is actually much more about infrastructure management

268
00:15:33,631 --> 00:15:35,935
If you come to the university to do computing

269
00:15:36,447 --> 00:15:40,799
You have to form a trio with somebody from the humanities and somebody from the pure sciences

270
00:15:41,311 --> 00:15:43,615
on your given task by the University every three months

271
00:15:45,407 --> 00:15:49,759
What we're doing is building a cross silent(solo?) network of people who focus on a task

272
00:15:50,271 --> 00:15:53,343
So that we can bring things together in novel or unexpected ways

273
00:15:54,623 --> 00:15:58,975
And a lot of this is about creating an architecture for Discovery

274
00:15:59,487 --> 00:16:02,047
rather than creating an architectures to deliver

275
00:16:02,303 --> 00:16:03,071
What people have asked for

276
00:16:04,351 --> 00:16:06,655
Did you get the discovery architecture right

277
00:16:06,911 --> 00:16:08,703
The recipe comes a lot easier

278
00:16:08,959 --> 00:16:11,775
Because you’ve got actually more structure in terms of the way you work

279
00:16:13,311 --> 00:16:16,895
I could go on on that but there's a lot more material that which I can move on to

280
00:16:17,407 --> 00:16:19,711
The key thing I want to finish off is this question and modelling

281
00:16:20,991 --> 00:16:23,295
Are you a fine people in complexity theory

282
00:16:23,551 --> 00:16:26,623
This is what I call the computational complexity guide(?)

283
00:16:27,135 --> 00:16:28,927
My field is called down through complexity

284
00:16:29,183 --> 00:16:30,975
And the study of complexity in human systems

285
00:16:31,743 --> 00:16:34,559
The computational guide(?) think everything is like birth locking

286
00:16:35,583 --> 00:16:38,655
You know but it's flock around in skies called boys algorithm

287
00:16:38,911 --> 00:16:41,471
It's follow the next bird max speed avoid collision

288
00:16:42,495 --> 00:16:44,287
and that also works by the way if you want

289
00:16:44,543 --> 00:16:46,079
To drive south of Naples in Italy

290
00:16:46,847 --> 00:16:51,711
It's a much more effective heuristic than following the rules, alright? if you match speed avoid collision

291
00:16:52,223 --> 00:16:58,367
Yeah and follow the next car you actually have a stress field driving experience even in Sorrento if you assume they’ll follow the rules

292
00:16:58,623 --> 00:16:59,391
forget it, alright?

293
00:16:59,903 --> 00:17:00,927
That's not the way it works

294
00:17:01,695 --> 00:17:03,487
The trouble being that human beings aren’t ants
302
00:17:03,743 --> 00:17:06,047
Because we haven't got, you know, you can predict

295
00:17:06,303 --> 00:17:08,607
The entire complexity of the termites nest

296
00:17:09,631 --> 00:17:14,495
This is genetically encoded rules responding consistently to (?) them stimulation

297
00:17:15,519 --> 00:17:18,079
The human beings can see things through metaphors

298
00:17:18,847 --> 00:17:20,127
We see things for abstraction

299
00:17:20,895 --> 00:17:26,527
We can change identity overnight when I'm with my children I'm a completely different person than on stage

300
00:17:27,807 --> 00:17:32,415
We don't have single agency we don't have single rules with complex pattern-based creatures

301
00:17:33,183 --> 00:17:34,975
So, basically, human beings aren’t ants
310
00:17:35,743 --> 00:17:38,047
And that means you’ve got a real problem with modelling

302
00:17:39,327 --> 00:17:45,471
I gave the keynote at the European complexity conference all academics they said what I speak about why you can't model of Complex Adaptive system

303
00:17:45,727 --> 00:17:51,103
I said no problem nobody told me the entire audience will be academics who built models

304
00:17:52,383 --> 00:17:57,759
that would like Daniel going into the Lions Den that was when I met Gelman he was the other here

305
00:17:58,015 --> 00:18:00,831
And you can't challenge Gelman he’s won the world's most famous physicist

306
00:18:01,343 --> 00:18:04,159
He backed me up and said you can't model the human system

307
00:18:05,695 --> 00:18:08,767
The only valid model of human system is a system itself

308
00:18:09,535 --> 00:18:14,143
A map to some extent is where we're starting to go but it's also part of the general concern

309
00:18:14,911 --> 00:18:17,215
About an increase of artificial intelligence 

310
00:18:17,471 --> 00:18:17,983
Focus on artificial intelligence 

311
00:18:19,519 --> 00:18:24,383
This is actually real problems, anybody hasn't ready the IAIN M. BANKS’ book: surface detail?

312
00:18:24,895 --> 00:18:29,503
IAIN M. BANKS is a brilliant science fiction writer, and tragically died of cancer

313
00:18:30,015 --> 00:18:35,391
And he then proposed to his long-term girlfriend by asking her if she would do the honor of becoming his widow

314
00:18:36,159 --> 00:18:38,975
Is that sort of sense of humor, right?

315
00:18:39,743 --> 00:18:44,095
Read that and be terrified I've just been involved in a big think tank in California

316
00:18:44,351 --> 00:18:47,679
General Hughes act is actually AI is the second most

317
00:18:47,935 --> 00:18:49,215
Highest existential threat to

318
00:18:49,471 --> 00:18:51,263
humanity after nuclear war

319
00:18:51,775 --> 00:18:52,799
And ecological

320
00:18:53,055 --> 00:18:54,335
Change is actually number four

321
00:18:55,615 --> 00:19:00,735
where the reason is we’re reducing human beings to information retrieval and process

322
00:19:01,247 --> 00:19:02,527
And AI is better than that

323
00:19:04,063 --> 00:19:06,879
And we won't go on to AI driven more machines and things like that

324
00:19:07,135 --> 00:19:10,463
We need to be very careful about that because a human being is more than this

325
00:19:11,231 --> 00:19:13,279
And then of course we get all the culture nonsense

326
00:19:15,071 --> 00:19:17,887
Everybody wants to engineer if in doubt blame culture

327
00:19:18,655 --> 00:19:21,471
If your pet project didn't work it was their fault

328
00:19:22,239 --> 00:19:25,567
Because they have the wrong culture or the wrong mindset, now this is

329
00:19:25,823 --> 00:19:27,103
Appalling science

330
00:19:28,127 --> 00:19:30,431
There is no such thing as a mindset

331
00:19:31,455 --> 00:19:34,527
There Is a complex set of interactions overtimes in a series of

332
00:19:34,783 --> 00:19:35,295
The tropes

333
00:19:35,551 --> 00:19:37,343
But it's not an individual mindset

334
00:19:38,367 --> 00:19:42,463
You get people who will gather people’s stories that's good news we gather people’s stories

335
00:19:43,231 --> 00:19:43,743
But we let

336
00:19:43,999 --> 00:19:46,047
People interpret them themselves

337
00:19:46,815 --> 00:19:50,399
We don't interpret them or let computers interpret them

338
00:19:50,911 --> 00:19:52,703
Because that restricts us to text

339
00:19:53,727 --> 00:19:55,775
And by the way you can write down

340
00:19:56,031 --> 00:19:58,079
10% of what you know

341
00:19:59,615 --> 00:20:01,919
Anything which is processing text

342
00:20:02,687 --> 00:20:05,503
Is again dealing with a very very limited set

343
00:20:06,783 --> 00:20:08,575
There's a power issue here as well

344
00:20:08,831 --> 00:20:11,391
Who interprets people's narrative is there

345
00:20:11,647 --> 00:20:13,951
This is from work were just doing with the BBC

346
00:20:14,463 --> 00:20:16,255
Weather presenting cartoons

347
00:20:16,511 --> 00:20:19,583
Employees are choosing cartoons which represent their culture

348
00:20:20,351 --> 00:20:23,679
Interpreting those on the high abstraction metadata

349
00:20:23,935 --> 00:20:27,007
and from that we then actually drawing a series of maps

350
00:20:27,519 --> 00:20:30,079
And when the reason is we don't want to fall into one of these traps

351
00:20:31,871 --> 00:20:34,943
Yep mansplaining post-colonialism

352
00:20:35,455 --> 00:20:38,783
All that lovely word about the expert must be right

353
00:20:39,295 --> 00:20:39,807
Ok

354
00:20:40,063 --> 00:20:41,599
I'm giving you three illustrations there

355
00:20:42,367 --> 00:20:44,671
If you really want to understand the human system

356
00:20:45,183 --> 00:20:47,743
You’ve got a hand over the power of interpretation

357
00:20:48,255 --> 00:20:49,791
To the people themselves

358
00:20:50,047 --> 00:20:51,839
Does it mean you don't have a role for experts

359
00:20:52,351 --> 00:20:55,679
We got to distribute the cognitive function you can't hold that centrally

360
00:20:56,447 --> 00:20:57,471
So from that

361
00:20:58,239 --> 00:20:59,775
That allows us to do some very interest things

362
00:21:01,567 --> 00:21:03,871
This example is a dispositional map

363
00:21:04,127 --> 00:21:06,943
And this is actually where we are starting to go with systems architecture

364
00:21:07,455 --> 00:21:08,991
Also with things like kanban

365
00:21:10,015 --> 00:21:11,807
So if you look at categories and Kanban(?)

366
00:21:12,831 --> 00:21:15,647
Ok the key Concepts on kanban is working progress

367
00:21:16,671 --> 00:21:22,815
Yeah not the representation of those to hear the advocates you would think it was all about Post-it notes moving between columns

368
00:21:23,583 --> 00:21:25,119
Key Concepts is working progress

369
00:21:25,887 --> 00:21:29,727
In an ordered system work in progress is a series of discrete job unit

370
00:21:29,983 --> 00:21:31,007
So The kanban Works

371
00:21:31,775 --> 00:21:35,103
In a complex system is a series of unrealized

372
00:21:35,359 --> 00:21:36,127
Potentialities

373
00:21:36,639 --> 00:21:38,431
So the representation has to change

374
00:21:39,199 --> 00:21:45,087
This is actually a landscape drawn from mass observation self interpretation at the point of origin

375
00:21:45,599 --> 00:21:49,695
And this is actually a cultural change map so if I actually want to change culture on this one

376
00:21:50,463 --> 00:21:52,511
If my desires Stacey(?) is up here

377
00:21:54,559 --> 00:21:56,351
You can see I've got some things up there so

378
00:21:56,607 --> 00:21:57,119
It’s a tick in the box

379
00:21:57,631 --> 00:21:59,167
This is very very negative

380
00:21:59,679 --> 00:22:01,727
To move them up there as a Step Too Far

381
00:22:03,263 --> 00:22:05,311
What I do instead is I look at this

382
00:22:05,823 --> 00:22:07,615
And I say what can I do tomorrow

383
00:22:08,127 --> 00:22:09,407
To create more like these

384
00:22:10,175 --> 00:22:10,943
If you would like those

385
00:22:12,735 --> 00:22:15,551
What I'm doing this is a key principle of complex design

386
00:22:16,063 --> 00:22:18,879
Is I’m shifting the system to an adjacent possible

387
00:22:20,159 --> 00:22:22,463
And once I get enough stability at that point

388
00:22:22,719 --> 00:22:24,511
I can use more conventional approaches

389
00:22:25,023 --> 00:22:28,863
This is what I'm talking about architecting to under for discovery

390
00:22:29,119 --> 00:22:30,911
Before you architect for delivery

391
00:22:31,935 --> 00:22:34,495
And if you actually start with the delivery

392
00:22:34,751 --> 00:22:36,543
You’ve missed the discovery phase

393
00:22:36,799 --> 00:22:38,847
and you’re gonna miss opportunities as well as threats

394
00:22:39,871 --> 00:22:41,663
That's actually from a hospital

395
00:22:42,175 --> 00:22:45,247
That also allows us a key Concept called fractal engagement

396
00:22:46,015 --> 00:22:51,135
So the same data can produce frameworks at the level of responsibility of every individual

397
00:22:52,159 --> 00:22:55,743
And again, I haven't got time to go into this I'll go this in the strategy session

398
00:22:56,255 --> 00:22:59,327
Fractal engagement is the way we actually achieve change

399
00:23:00,095 --> 00:23:03,167
People don't make decisions about what other people should do

400
00:23:03,423 --> 00:23:08,031
They make decisions about what they can do tomorrow within their own sphere of competence

401
00:23:09,311 --> 00:23:12,127
Then the system as a whole orientates through multiple actions

402
00:23:12,639 --> 00:23:14,943
That's a big concept I’m just hinting at it now

403
00:23:15,711 --> 00:23:17,503
And that’s what we call fractal change and way

404
00:23:19,295 --> 00:23:20,831
It's also about multiple perspectives

405
00:23:22,623 --> 00:23:26,207
This is actually from again this is work on the whole variety of areas.

406
00:23:26,463 --> 00:23:27,743
This is Peace and Conflict resolution

407
00:23:28,511 --> 00:23:33,887
What we doing here is presenting in infographic and we are doing this in system design Discovery architecture

408
00:23:34,399 --> 00:23:37,471
We’re Representing infographics about where we think we should be

409
00:23:38,239 --> 00:23:40,799
We're getting every employee to interpret that

410
00:23:41,055 --> 00:23:45,663
And drawing the maps of the interpretation before anybody talks to anybody else

411
00:23:47,711 --> 00:23:50,271
In that space is actually from Columbia this is

412
00:23:50,783 --> 00:23:51,551
Yeah big one

413
00:23:52,575 --> 00:23:57,183
You can see that blue and green see the same data in such radically different ways

414
00:23:57,439 --> 00:23:59,999
Sitting them down to talk to each other as a waste of time

415
00:24:01,279 --> 00:24:03,583
The only thing they both agree on this is they hate the red guys

416
00:24:05,375 --> 00:24:06,655
the strategy here

417
00:24:07,935 --> 00:24:09,983
Is there actually make these more extreme

418
00:24:11,775 --> 00:24:13,823
So that we get a new consensus in the middle

419
00:24:15,103 --> 00:24:19,199
The other reason we do this is I want to find outlets

420
00:24:19,967 --> 00:24:21,759
Again I talk about Discovery architecture

421
00:24:22,271 --> 00:24:27,135
I want to see groups of people who are seeing the world differently from everybody else

422
00:24:27,647 --> 00:24:30,207
Because those are the things the 17%

423
00:24:31,231 --> 00:24:32,511
Who seen the Cancer module

424
00:24:33,023 --> 00:24:37,375
Rather than people are going with orthodoxy so again that's the concept of Discovery architecture

425
00:24:37,887 --> 00:24:38,911
Rather than delivery architect

426
00:24:40,191 --> 00:24:43,263
And the final slide finish off on this How You scale a complex system

427
00:24:44,543 --> 00:24:49,663
Again this is a big field, but the one thing you don't do is you never scale the complex adaptor

428
00:24:49,919 --> 00:24:50,431
system

429
00:24:50,687 --> 00:24:52,991
By aggregation or imitation

430
00:24:54,783 --> 00:24:58,111
You don't take everything and aggregated it up into some massive diagram

431
00:24:58,623 --> 00:25:00,927
And you don't say that worked will do more of it

432
00:25:02,463 --> 00:25:08,607
What do you actually do is you decompose things and there's a whole body of working on this, by the way, an object orientation

433
00:25:08,863 --> 00:25:10,399
Which some of us are going back to at the moment

434
00:25:10,911 --> 00:25:14,239
You decompose things to the lowest coherent object

435
00:25:15,263 --> 00:25:17,823
You define protocols for the interactions 

436
00:25:18,079 --> 00:25:18,847
Between the objects

437
00:25:19,615 --> 00:25:22,687
And you allow novel patterns to emerge as you stimulator the system

438
00:25:23,711 --> 00:25:28,063
So you sail but scale by decomposition and recombination

439
00:25:28,831 --> 00:25:30,367
Not by aggregation

440
00:25:30,879 --> 00:25:32,415
And not by imitation

441
00:25:33,439 --> 00:25:36,511
Have a challenge for design the Challenge for architecture

442
00:25:36,767 --> 00:25:39,327
Is to move much further back in the cycle

443
00:25:40,095 --> 00:25:42,143
To deal with an articulated needs

444
00:25:43,167 --> 00:25:45,215
To discover on an objective basis

445
00:25:45,471 --> 00:25:48,287
Novel possibilities unexpected consequences

446
00:25:48,799 --> 00:25:51,615
And build systems with the responsive capability

447
00:25:52,127 --> 00:25:54,943
To exploit opportunities as they present themselves

448
00:25:55,455 --> 00:25:56,479
Before they're fully now

449
00:25:57,503 --> 00:26:01,599
In order to do that we need to have a sound understanding of the science

450
00:26:02,111 --> 00:26:03,391
Behind human cognition

451
00:26:04,159 --> 00:26:05,695
And the science behind systems

452
00:26:06,719 --> 00:26:09,279
Based on the conditions of extreme uncertainty

453
00:26:09,791 --> 00:26:13,375
Falling back to the constraints of natural science gives us security

454
00:26:14,143 --> 00:26:17,983
Falling back to empirical parsh memories of what work before

455
00:26:18,495 --> 00:26:20,543
Makes us very very vulnerable

456
00:26:21,311 --> 00:26:22,079
Thank you very much for your time
